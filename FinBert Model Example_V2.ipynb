{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook contains demo for our fine-tuned Analyst Tone model. We fine-tuned FinBERT model on 10,000 manually annotated analyst statements. You can use this script and infer sentiment on your customerized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-12T03:42:27.110781Z",
     "start_time": "2020-09-12T03:42:27.106997Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# download pre-trained and fine-tuned weights, unzip to the working directory\n",
    "# https://gohkust-my.sharepoint.com/:u:/g/personal/imyiyang_ust_hk/EQJGiEOkhIlBqlW63TbKA3gBCYgDDcHlBCB7VTXIUMmyiA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.3 (default, Mar 27 2019, 16:54:48) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:03:02.440809Z",
     "start_time": "2020-11-14T03:03:00.771508Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertConfig\n",
    "from bertModel import BertClassification, dense_opt\n",
    "from datasets import text_dataset, financialPhraseBankDataset\n",
    "import argparse\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:03:02.447859Z",
     "start_time": "2020-11-14T03:03:02.443270Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "labels = {0:'neutral', 1:'positive',2:'negative'}\n",
    "num_labels= len(labels)\n",
    "vocab =\"finance-uncased\"\n",
    "vocab_path = '/Users/svetlana/Downloads/analyst_tone-2/vocab' \n",
    "pretrained_weights_path ='/Users/svetlana/Downloads/analyst_tone-2/pretrained_weights'\n",
    "fine_tuned_weight_path = '/Users/svetlana/Downloads/analyst_tone-2/fine_tuned.pth'   \n",
    "max_seq_length=256\n",
    "device=torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:03:04.572108Z",
     "start_time": "2020-11-14T03:03:02.450447Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/Users/svetlana/finBERT/from git/bertModel.py:31: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(self.classifier.weight)\n"
     ]
    }
   ],
   "source": [
    "model = BertClassification(weight_path = pretrained_weights_path, \n",
    "                           num_labels=num_labels, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "bert.embeddings.word_embeddings.weight \t torch.Size([30873, 768])\n",
      "bert.embeddings.position_embeddings.weight \t torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight \t torch.Size([2, 768])\n",
      "bert.embeddings.LayerNorm.weight \t torch.Size([768])\n",
      "bert.embeddings.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.pooler.dense.weight \t torch.Size([768, 768])\n",
      "bert.pooler.dense.bias \t torch.Size([768])\n",
      "classifier.weight \t torch.Size([3, 768])\n",
      "classifier.bias \t torch.Size([3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:03:10.199945Z",
     "start_time": "2020-11-14T03:03:04.574714Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30873, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(fine_tuned_weight_path, map_location=device))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-12T02:55:51.929515Z",
     "start_time": "2020-09-12T02:55:51.927166Z"
    }
   },
   "source": [
    "# 0 is neutral, 1 is positive, and 2 is negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:03:53.760997Z",
     "start_time": "2020-11-14T03:03:53.755690Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "text_path = '/Users/svetlana/Desktop/Creation/Ventas 06112020'\n",
    "files = glob.glob(text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        sentences = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\ufeffHitachi and SBI, the largest state-owned commercial bank in India,\\n',\n",
       " 'enter into Joint Venture to accelerate Digital Payments in India\\n',\n",
       " 'Partnership to establish a state-of-the-art card acceptance\\n',\n",
       " 'and future ready digital payments platform\\n',\n",
       " 'MUMBAI, October 29, 2018 --- Hitachi, Ltd. (TSE: 6501, \"Hitachi\") today announced that Hitachi Payment Services Pvt. Ltd. (\"Hitachi Payments\"), a wholly-owned subsidiary based in India of Hitachi, and State Bank of India (\"SBI\") have signed a definitive agreement to enter into a joint venture for the establishment of a state-of-the-art card acceptance and future ready digital payments platform for India.\\n',\n",
       " 'It is planned that Hitachi Payments will invest [26%] to SBI Payment Services Pvt. Ltd. (\"SBI Payment\"), a wholly-owned subsidiary of SBI, and through this investment, SBI Payment will be a joint venture between both parties. Both parties will proceed to apply for regulatory approvals. \\n',\n",
       " 'Financial services market in India is making remarkable progress led by economic growth, Financial Inclusion policy*1 and Digital India initiatives. Bank account holders have increased substantially in the past few years and as a result, banking transactions on ATMs and digital transactions have correspondingly increased dramatically.\\n',\n",
       " 'SBI, as the largest state-owned commercial bank in India, has more than 420 million customers and more than 6,00,000 POS*2 terminals and is today the largest merchant acquirer in the market in terms of terminals through its subsidiary SBI Payment.\\n',\n",
       " 'Hitachi Payments empowers financial institutions with a comprehensive array of technology-led cash and digital payment solutions such as ATM managed services*3 and POS processing services*4. It has over 55,000 ATMs and 850,000 POS devices (including Mobile POS) under management in India. Hitachi Payments has been providing deployment, technology and management services for the card and digital acceptance payment network of SBI since 2011.\\n',\n",
       " \"Through this joint venture, Hitachi enters a new field of business which is the merchant acquiring business*5 in India. Hitachi will contribute to the development and expansion of digital payments service business in India by creating a digital payments platform that will enable better convenience and quality through integrating our individual strengths, SBI's large customer base, branch network, brand trust and Hitachi's capability and know-how of digital payments and state-of-the-art digital technologies such as Big Data Analytics and Artificial Intelligence (AI).\\n\",\n",
       " 'Hitachi will provide wide range of services contributing to \"Digital India\" by creating innovative solutions with \"Lumada\". With this JV, Hitachi will accelerate digitalization of financial services in India by linking up digital payments platform to state-of-the-art digital technologies of \"Lumada\", and also will provide solutions for mass transit sector and e-commerce businesses. \\n',\n",
       " 'Hitachi has also been contributing to digitalize governmental and educational services such as \"e-Governance\" and \"e-Education\" mainly through Hitachi MGRM Net*6 which was established in April 2018 and provides IT services in India. and leverage considerable knowledge for our global business.\\n']"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:03:54.277505Z",
     "start_time": "2020-11-14T03:03:54.217928Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer(vocab_file = vocab_path, do_lower_case = True, do_basic_tokenize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:08:32.253204Z",
     "start_time": "2020-11-14T03:08:32.121884Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿Hitachi and SBI, the largest state-owned commercial bank in India,\n",
      " \n",
      "FinBERT predicted sentiment:  neutral \n",
      "\n",
      "enter into Joint Venture to accelerate Digital Payments in India\n",
      " \n",
      "FinBERT predicted sentiment:  neutral \n",
      "\n",
      "Partnership to establish a state-of-the-art card acceptance\n",
      " \n",
      "FinBERT predicted sentiment:  neutral \n",
      "\n",
      "and future ready digital payments platform\n",
      " \n",
      "FinBERT predicted sentiment:  positive \n",
      "\n",
      "MUMBAI, October 29, 2018 --- Hitachi, Ltd. (TSE: 6501, \"Hitachi\") today announced that Hitachi Payment Services Pvt. Ltd. (\"Hitachi Payments\"), a wholly-owned subsidiary based in India of Hitachi, and State Bank of India (\"SBI\") have signed a definitive agreement to enter into a joint venture for the establishment of a state-of-the-art card acceptance and future ready digital payments platform for India.\n",
      " \n",
      "FinBERT predicted sentiment:  neutral \n",
      "\n",
      "It is planned that Hitachi Payments will invest [26%] to SBI Payment Services Pvt. Ltd. (\"SBI Payment\"), a wholly-owned subsidiary of SBI, and through this investment, SBI Payment will be a joint venture between both parties. Both parties will proceed to apply for regulatory approvals. \n",
      " \n",
      "FinBERT predicted sentiment:  neutral \n",
      "\n",
      "Financial services market in India is making remarkable progress led by economic growth, Financial Inclusion policy*1 and Digital India initiatives. Bank account holders have increased substantially in the past few years and as a result, banking transactions on ATMs and digital transactions have correspondingly increased dramatically.\n",
      " \n",
      "FinBERT predicted sentiment:  positive \n",
      "\n",
      "SBI, as the largest state-owned commercial bank in India, has more than 420 million customers and more than 6,00,000 POS*2 terminals and is today the largest merchant acquirer in the market in terms of terminals through its subsidiary SBI Payment.\n",
      " \n",
      "FinBERT predicted sentiment:  neutral \n",
      "\n",
      "Hitachi Payments empowers financial institutions with a comprehensive array of technology-led cash and digital payment solutions such as ATM managed services*3 and POS processing services*4. It has over 55,000 ATMs and 850,000 POS devices (including Mobile POS) under management in India. Hitachi Payments has been providing deployment, technology and management services for the card and digital acceptance payment network of SBI since 2011.\n",
      " \n",
      "FinBERT predicted sentiment:  neutral \n",
      "\n",
      "Through this joint venture, Hitachi enters a new field of business which is the merchant acquiring business*5 in India. Hitachi will contribute to the development and expansion of digital payments service business in India by creating a digital payments platform that will enable better convenience and quality through integrating our individual strengths, SBI's large customer base, branch network, brand trust and Hitachi's capability and know-how of digital payments and state-of-the-art digital technologies such as Big Data Analytics and Artificial Intelligence (AI).\n",
      " \n",
      "FinBERT predicted sentiment:  positive \n",
      "\n",
      "Hitachi will provide wide range of services contributing to \"Digital India\" by creating innovative solutions with \"Lumada\". With this JV, Hitachi will accelerate digitalization of financial services in India by linking up digital payments platform to state-of-the-art digital technologies of \"Lumada\", and also will provide solutions for mass transit sector and e-commerce businesses. \n",
      " \n",
      "FinBERT predicted sentiment:  positive \n",
      "\n",
      "Hitachi has also been contributing to digitalize governmental and educational services such as \"e-Governance\" and \"e-Education\" mainly through Hitachi MGRM Net*6 which was established in April 2018 and provides IT services in India. and leverage considerable knowledge for our global business.\n",
      " \n",
      "FinBERT predicted sentiment:  positive \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for sent in sentences: \n",
    "    tokenized_sent = tokenizer.tokenize(sent)\n",
    "    if len(tokenized_sent) > max_seq_length:\n",
    "        tokenized_sent = tokenized_sent[:max_seq_length]\n",
    "    \n",
    "    ids_review  = tokenizer.convert_tokens_to_ids(tokenized_sent)\n",
    "    mask_input = [1]*len(ids_review)        \n",
    "    padding = [0] * (max_seq_length - len(ids_review))\n",
    "    ids_review += padding\n",
    "    mask_input += padding\n",
    "    input_type = [0]*max_seq_length\n",
    "    \n",
    "    input_ids = torch.tensor(ids_review).to(device).reshape(-1, 256)\n",
    "    attention_mask =  torch.tensor(mask_input).to(device).reshape(-1, 256)\n",
    "    token_type_ids = torch.tensor(input_type).to(device).reshape(-1, 256)\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model(input_ids, token_type_ids, attention_mask)\n",
    "        outputs = F.softmax(outputs,dim=1)\n",
    "        print(sent, '\\nFinBERT predicted sentiment: ', labels[torch.argmax(outputs).item()], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "os.getcwd()\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "dirpath = r'/Users/svetlana/Desktop/Creation/'\n",
    "out_file = 'log_creation.csv'\n",
    "with open(out_file, 'w') as outfile:\n",
    "    csvout = csv.writer(outfile)\n",
    "\n",
    "    files = [name for name in os.listdir(dirpath)\n",
    "        if name.endswith('txt')]\n",
    "\n",
    "    for file in files:\n",
    "        with open(dirpath + '/' + file) as in_file:\n",
    "            stripped = (line.strip() for line in in_file)\n",
    "            lines = (line.split(\",\") for line in stripped if line)\n",
    "            csvout.writerow([\" \".join([line.strip() for line in in_file])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/svetlana/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/svetlana/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "en_stop.add('joint')\n",
    "en_stop.add('venture')\n",
    "en_stop.add(' ')\n",
    "print(len(en_stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "text_data = []\n",
    "with open(out_file) as f:\n",
    "    for line in f:\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        text_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(text_data)\n",
    "dictionary.filter_extremes(no_below=6, no_above=1.0)\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.020*\"technology\" + 0.018*\"company\" + 0.017*\"business\" + 0.015*\"million\" + 0.015*\"production\" + 0.015*\"partner\" + 0.014*\"investment\" + 0.013*\"create\" + 0.013*\"market\" + 0.010*\"provide\"')\n",
      "(1, '0.055*\"company\" + 0.029*\"combine\" + 0.021*\"customer\" + 0.021*\"transaction\" + 0.020*\"business\" + 0.019*\"billion\" + 0.015*\"expect\" + 0.014*\"create\" + 0.014*\"share\" + 0.012*\"include\"')\n",
      "(2, '0.040*\"investment\" + 0.025*\"project\" + 0.021*\"development\" + 0.018*\"capital\" + 0.017*\"market\" + 0.015*\"solution\" + 0.015*\"partner\" + 0.013*\"credit\" + 0.013*\"provide\" + 0.012*\"partnership\"')\n",
      "(3, '0.033*\"energy\" + 0.026*\"facility\" + 0.025*\"technology\" + 0.022*\"production\" + 0.020*\"product\" + 0.019*\"business\" + 0.019*\"agreement\" + 0.018*\"company\" + 0.016*\"develop\" + 0.016*\"capacity\"')\n",
      "(4, '0.050*\"services\" + 0.044*\"payment\" + 0.038*\"medium\" + 0.038*\"digital\" + 0.029*\"company\" + 0.022*\"group\" + 0.021*\"platform\" + 0.020*\"technology\" + 0.018*\"financial\" + 0.018*\"business\"')\n",
      "(5, '0.046*\"brand\" + 0.046*\"consumer\" + 0.027*\"chinese\" + 0.026*\"market\" + 0.026*\"global\" + 0.023*\"partnership\" + 0.023*\"china\" + 0.021*\"technology\" + 0.020*\"agreement\" + 0.019*\"million\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 6\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, random_state=1, passes=50, alpha = 'auto')\n",
    "topics = ldamodel.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
    "#import pyLDAvis.gensim\n",
    "#lda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=True)\n",
    "#pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.020*\"technology\" + 0.018*\"company\" + 0.017*\"business\" + 0.015*\"million\" + 0.015*\"production\" + 0.015*\"partner\" + 0.014*\"investment\" + 0.013*\"create\" + 0.013*\"market\" + 0.010*\"provide\"'), (1, '0.055*\"company\" + 0.029*\"combine\" + 0.021*\"customer\" + 0.021*\"transaction\" + 0.020*\"business\" + 0.019*\"billion\" + 0.015*\"expect\" + 0.014*\"create\" + 0.014*\"share\" + 0.012*\"include\"'), (2, '0.040*\"investment\" + 0.025*\"project\" + 0.021*\"development\" + 0.018*\"capital\" + 0.017*\"market\" + 0.015*\"solution\" + 0.015*\"partner\" + 0.013*\"credit\" + 0.013*\"provide\" + 0.012*\"partnership\"'), (3, '0.033*\"energy\" + 0.026*\"facility\" + 0.025*\"technology\" + 0.022*\"production\" + 0.020*\"product\" + 0.019*\"business\" + 0.019*\"agreement\" + 0.018*\"company\" + 0.016*\"develop\" + 0.016*\"capacity\"'), (4, '0.050*\"services\" + 0.044*\"payment\" + 0.038*\"medium\" + 0.038*\"digital\" + 0.029*\"company\" + 0.022*\"group\" + 0.021*\"platform\" + 0.020*\"technology\" + 0.018*\"financial\" + 0.018*\"business\"'), (5, '0.046*\"brand\" + 0.046*\"consumer\" + 0.027*\"chinese\" + 0.026*\"market\" + 0.026*\"global\" + 0.023*\"partnership\" + 0.023*\"china\" + 0.021*\"technology\" + 0.020*\"agreement\" + 0.019*\"million\"')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.020*\"technology\" + 0.018*\"company\" + 0.017*\"business\" + 0.015*\"million\" + 0.015*\"production\" + 0.015*\"partner\" + 0.014*\"investment\" + 0.013*\"create\" + 0.013*\"market\" + 0.010*\"provide\"'),\n",
       " (1,\n",
       "  '0.055*\"company\" + 0.029*\"combine\" + 0.021*\"customer\" + 0.021*\"transaction\" + 0.020*\"business\" + 0.019*\"billion\" + 0.015*\"expect\" + 0.014*\"create\" + 0.014*\"share\" + 0.012*\"include\"'),\n",
       " (2,\n",
       "  '0.040*\"investment\" + 0.025*\"project\" + 0.021*\"development\" + 0.018*\"capital\" + 0.017*\"market\" + 0.015*\"solution\" + 0.015*\"partner\" + 0.013*\"credit\" + 0.013*\"provide\" + 0.012*\"partnership\"'),\n",
       " (3,\n",
       "  '0.033*\"energy\" + 0.026*\"facility\" + 0.025*\"technology\" + 0.022*\"production\" + 0.020*\"product\" + 0.019*\"business\" + 0.019*\"agreement\" + 0.018*\"company\" + 0.016*\"develop\" + 0.016*\"capacity\"'),\n",
       " (4,\n",
       "  '0.050*\"services\" + 0.044*\"payment\" + 0.038*\"medium\" + 0.038*\"digital\" + 0.029*\"company\" + 0.022*\"group\" + 0.021*\"platform\" + 0.020*\"technology\" + 0.018*\"financial\" + 0.018*\"business\"'),\n",
       " (5,\n",
       "  '0.046*\"brand\" + 0.046*\"consumer\" + 0.027*\"chinese\" + 0.026*\"market\" + 0.026*\"global\" + 0.023*\"partnership\" + 0.023*\"china\" + 0.021*\"technology\" + 0.020*\"agreement\" + 0.019*\"million\"')]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=NUM_TOPICS, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-5.577324668137916"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.log_perplexity(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3234008393505779\n"
     ]
    }
   ],
   "source": [
    "cm = CoherenceModel(model=ldamodel, texts=text_data, corpus=corpus, dictionary = dictionary, coherence='c_v')\n",
    "coherence = cm.get_coherence()\n",
    "print(coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "limit=30\n",
    "start=1\n",
    "step=1\n",
    "texts = text_data\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(1,30,1):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus, num_topics = num_topics, id2word=dictionary, passes=25, alpha = 'asymmetric')\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=text_data, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<gensim.models.ldamodel.LdaModel object at 0x7f93c8735080>, <gensim.models.ldamodel.LdaModel object at 0x7f93c87355c0>, <gensim.models.ldamodel.LdaModel object at 0x7f937af78048>, <gensim.models.ldamodel.LdaModel object at 0x7f937af6bf28>, <gensim.models.ldamodel.LdaModel object at 0x7f93e82e3da0>, <gensim.models.ldamodel.LdaModel object at 0x7f93c9f8d9e8>, <gensim.models.ldamodel.LdaModel object at 0x7f937af719e8>, <gensim.models.ldamodel.LdaModel object at 0x7f937af51f60>, <gensim.models.ldamodel.LdaModel object at 0x7f943d340160>, <gensim.models.ldamodel.LdaModel object at 0x7f937af63780>, <gensim.models.ldamodel.LdaModel object at 0x7f937af58978>, <gensim.models.ldamodel.LdaModel object at 0x7f93f86f4da0>, <gensim.models.ldamodel.LdaModel object at 0x7f93c9f6cdd8>, <gensim.models.ldamodel.LdaModel object at 0x7f937af31358>, <gensim.models.ldamodel.LdaModel object at 0x7f937af56470>, <gensim.models.ldamodel.LdaModel object at 0x7f937af517b8>, <gensim.models.ldamodel.LdaModel object at 0x7f937af64ac8>, <gensim.models.ldamodel.LdaModel object at 0x7f937af6d6d8>, <gensim.models.ldamodel.LdaModel object at 0x7f93c8516668>, <gensim.models.ldamodel.LdaModel object at 0x7f937af98fd0>, <gensim.models.ldamodel.LdaModel object at 0x7f937af6b780>, <gensim.models.ldamodel.LdaModel object at 0x7f937b1e8588>, <gensim.models.ldamodel.LdaModel object at 0x7f938015d710>, <gensim.models.ldamodel.LdaModel object at 0x7f93f870aac8>, <gensim.models.ldamodel.LdaModel object at 0x7f93e82e39b0>, <gensim.models.ldamodel.LdaModel object at 0x7f939842a160>, <gensim.models.ldamodel.LdaModel object at 0x7f93f86f9d68>, <gensim.models.ldamodel.LdaModel object at 0x7f939842a2e8>, <gensim.models.ldamodel.LdaModel object at 0x7f93e3899a90>]\n",
      "[0.19307018902136402, 0.3013907771407553, 0.29321321412490065, 0.30375371353829583, 0.4134491947620796, 0.3376810002079756, 0.33654237061885706, 0.32637867767807627, 0.3429881892156501, 0.32705215514232444, 0.3473240358898371, 0.32786247718915174, 0.3347748564531679, 0.3235041958911125, 0.342057493291043, 0.33679596018732505, 0.3360645753455857, 0.3373488383203797, 0.2871468747070635, 0.3444163387673035, 0.31664618663443794, 0.3366049316145638, 0.33207200272349885, 0.3223118170984076, 0.3285645828338737, 0.33654048596244507, 0.3059192317487271, 0.3178066752483016, 0.3158336442765304]\n"
     ]
    }
   ],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=text_data, start=1, \n",
    "                                                        limit=10, step=1)\n",
    "print(model_list)\n",
    "print(coherence_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = range(1,30,1)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"cv\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting financial data with yahoo_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import yahoo_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "from yahoo_fin.stock_info import *\n",
    "ventas_data = get_data('VTR', start_date = '11/05/2020' , end_date = '11/10/2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-05    40.741283\n",
      "2020-11-06    39.274914\n",
      "2020-11-09    46.418510\n",
      "Name: adjclose, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(ventas_data['adjclose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finbert",
   "language": "python",
   "name": "finbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
