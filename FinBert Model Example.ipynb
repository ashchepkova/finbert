{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook contains demo for our fine-tuned Analyst Tone model. We fine-tuned FinBERT model on 10,000 manually annotated analyst statements. You can use this script and infer sentiment on your customerized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-12T03:42:27.110781Z",
     "start_time": "2020-09-12T03:42:27.106997Z"
    }
   },
   "outputs": [],
   "source": [
    "# download pre-trained and fine-tuned weights, unzip to the working directory\n",
    "# https://gohkust-my.sharepoint.com/:u:/g/personal/imyiyang_ust_hk/EQJGiEOkhIlBqlW63TbKA3gBCYgDDcHlBCB7VTXIUMmyiA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.3 (default, Mar 27 2019, 16:54:48) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:03:02.440809Z",
     "start_time": "2020-11-14T03:03:00.771508Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertConfig\n",
    "from bertModel import BertClassification, dense_opt\n",
    "from datasets import text_dataset, financialPhraseBankDataset\n",
    "import argparse\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:03:02.447859Z",
     "start_time": "2020-11-14T03:03:02.443270Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = {0:'neutral', 1:'positive',2:'negative'}\n",
    "num_labels= len(labels)\n",
    "vocab =\"finance-uncased\"\n",
    "vocab_path = '/Users/svetlana/Downloads/analyst_tone-2/vocab' \n",
    "pretrained_weights_path ='/Users/svetlana/Downloads/analyst_tone-2/pretrained_weights'\n",
    "fine_tuned_weight_path = '/Users/svetlana/Downloads/analyst_tone-2/fine_tuned.pth'   \n",
    "max_seq_length=256\n",
    "device=torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:03:04.572108Z",
     "start_time": "2020-11-14T03:03:02.450447Z"
    }
   },
   "outputs": [],
   "source": [
    "model = BertClassification(weight_path = pretrained_weights_path, \n",
    "                           num_labels=num_labels, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "bert.embeddings.word_embeddings.weight \t torch.Size([30873, 768])\n",
      "bert.embeddings.position_embeddings.weight \t torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight \t torch.Size([2, 768])\n",
      "bert.embeddings.LayerNorm.weight \t torch.Size([768])\n",
      "bert.embeddings.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias \t torch.Size([768])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.encoder.layer.11.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.pooler.dense.weight \t torch.Size([768, 768])\n",
      "bert.pooler.dense.bias \t torch.Size([768])\n",
      "classifier.weight \t torch.Size([3, 768])\n",
      "classifier.bias \t torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:03:10.199945Z",
     "start_time": "2020-11-14T03:03:04.574714Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30873, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(fine_tuned_weight_path, map_location=device))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-12T02:55:51.929515Z",
     "start_time": "2020-09-12T02:55:51.927166Z"
    }
   },
   "source": [
    "# 0 is neutral, 1 is positive, and 2 is negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:03:53.760997Z",
     "start_time": "2020-11-14T03:03:53.755690Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "text_path = '/Users/svetlana/Desktop/Creation/*'\n",
    "files = glob.glob(text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        sentences = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffHitachi and SBI, the largest state-owned commercial bank in India,\\n',\n",
       " 'enter into Joint Venture to accelerate Digital Payments in India\\n',\n",
       " 'Partnership to establish a state-of-the-art card acceptance\\n',\n",
       " 'and future ready digital payments platform\\n',\n",
       " 'MUMBAI, October 29, 2018 --- Hitachi, Ltd. (TSE: 6501, \"Hitachi\") today announced that Hitachi Payment Services Pvt. Ltd. (\"Hitachi Payments\"), a wholly-owned subsidiary based in India of Hitachi, and State Bank of India (\"SBI\") have signed a definitive agreement to enter into a joint venture for the establishment of a state-of-the-art card acceptance and future ready digital payments platform for India.\\n',\n",
       " 'It is planned that Hitachi Payments will invest [26%] to SBI Payment Services Pvt. Ltd. (\"SBI Payment\"), a wholly-owned subsidiary of SBI, and through this investment, SBI Payment will be a joint venture between both parties. Both parties will proceed to apply for regulatory approvals. \\n',\n",
       " 'Financial services market in India is making remarkable progress led by economic growth, Financial Inclusion policy*1 and Digital India initiatives. Bank account holders have increased substantially in the past few years and as a result, banking transactions on ATMs and digital transactions have correspondingly increased dramatically.\\n',\n",
       " 'SBI, as the largest state-owned commercial bank in India, has more than 420 million customers and more than 6,00,000 POS*2 terminals and is today the largest merchant acquirer in the market in terms of terminals through its subsidiary SBI Payment.\\n',\n",
       " 'Hitachi Payments empowers financial institutions with a comprehensive array of technology-led cash and digital payment solutions such as ATM managed services*3 and POS processing services*4. It has over 55,000 ATMs and 850,000 POS devices (including Mobile POS) under management in India. Hitachi Payments has been providing deployment, technology and management services for the card and digital acceptance payment network of SBI since 2011.\\n',\n",
       " \"Through this joint venture, Hitachi enters a new field of business which is the merchant acquiring business*5 in India. Hitachi will contribute to the development and expansion of digital payments service business in India by creating a digital payments platform that will enable better convenience and quality through integrating our individual strengths, SBI's large customer base, branch network, brand trust and Hitachi's capability and know-how of digital payments and state-of-the-art digital technologies such as Big Data Analytics and Artificial Intelligence (AI).\\n\",\n",
       " 'Hitachi will provide wide range of services contributing to \"Digital India\" by creating innovative solutions with \"Lumada\". With this JV, Hitachi will accelerate digitalization of financial services in India by linking up digital payments platform to state-of-the-art digital technologies of \"Lumada\", and also will provide solutions for mass transit sector and e-commerce businesses. \\n',\n",
       " 'Hitachi has also been contributing to digitalize governmental and educational services such as \"e-Governance\" and \"e-Education\" mainly through Hitachi MGRM Net*6 which was established in April 2018 and provides IT services in India. and leverage considerable knowledge for our global business.\\n']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:03:54.277505Z",
     "start_time": "2020-11-14T03:03:54.217928Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer(vocab_file = vocab_path, do_lower_case = True, do_basic_tokenize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:08:32.253204Z",
     "start_time": "2020-11-14T03:08:32.121884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿Hitachi and SBI, the largest state-owned commercial bank in India,\n",
      " \n",
      "FinBERT predicted sentiment:  neutral \n",
      "\n",
      "enter into Joint Venture to accelerate Digital Payments in India\n",
      " \n",
      "FinBERT predicted sentiment:  neutral \n",
      "\n",
      "Partnership to establish a state-of-the-art card acceptance\n",
      " \n",
      "FinBERT predicted sentiment:  neutral \n",
      "\n",
      "and future ready digital payments platform\n",
      " \n",
      "FinBERT predicted sentiment:  positive \n",
      "\n",
      "MUMBAI, October 29, 2018 --- Hitachi, Ltd. (TSE: 6501, \"Hitachi\") today announced that Hitachi Payment Services Pvt. Ltd. (\"Hitachi Payments\"), a wholly-owned subsidiary based in India of Hitachi, and State Bank of India (\"SBI\") have signed a definitive agreement to enter into a joint venture for the establishment of a state-of-the-art card acceptance and future ready digital payments platform for India.\n",
      " \n",
      "FinBERT predicted sentiment:  neutral \n",
      "\n",
      "It is planned that Hitachi Payments will invest [26%] to SBI Payment Services Pvt. Ltd. (\"SBI Payment\"), a wholly-owned subsidiary of SBI, and through this investment, SBI Payment will be a joint venture between both parties. Both parties will proceed to apply for regulatory approvals. \n",
      " \n",
      "FinBERT predicted sentiment:  neutral \n",
      "\n",
      "Financial services market in India is making remarkable progress led by economic growth, Financial Inclusion policy*1 and Digital India initiatives. Bank account holders have increased substantially in the past few years and as a result, banking transactions on ATMs and digital transactions have correspondingly increased dramatically.\n",
      " \n",
      "FinBERT predicted sentiment:  positive \n",
      "\n",
      "SBI, as the largest state-owned commercial bank in India, has more than 420 million customers and more than 6,00,000 POS*2 terminals and is today the largest merchant acquirer in the market in terms of terminals through its subsidiary SBI Payment.\n",
      " \n",
      "FinBERT predicted sentiment:  neutral \n",
      "\n",
      "Hitachi Payments empowers financial institutions with a comprehensive array of technology-led cash and digital payment solutions such as ATM managed services*3 and POS processing services*4. It has over 55,000 ATMs and 850,000 POS devices (including Mobile POS) under management in India. Hitachi Payments has been providing deployment, technology and management services for the card and digital acceptance payment network of SBI since 2011.\n",
      " \n",
      "FinBERT predicted sentiment:  neutral \n",
      "\n",
      "Through this joint venture, Hitachi enters a new field of business which is the merchant acquiring business*5 in India. Hitachi will contribute to the development and expansion of digital payments service business in India by creating a digital payments platform that will enable better convenience and quality through integrating our individual strengths, SBI's large customer base, branch network, brand trust and Hitachi's capability and know-how of digital payments and state-of-the-art digital technologies such as Big Data Analytics and Artificial Intelligence (AI).\n",
      " \n",
      "FinBERT predicted sentiment:  positive \n",
      "\n",
      "Hitachi will provide wide range of services contributing to \"Digital India\" by creating innovative solutions with \"Lumada\". With this JV, Hitachi will accelerate digitalization of financial services in India by linking up digital payments platform to state-of-the-art digital technologies of \"Lumada\", and also will provide solutions for mass transit sector and e-commerce businesses. \n",
      " \n",
      "FinBERT predicted sentiment:  positive \n",
      "\n",
      "Hitachi has also been contributing to digitalize governmental and educational services such as \"e-Governance\" and \"e-Education\" mainly through Hitachi MGRM Net*6 which was established in April 2018 and provides IT services in India. and leverage considerable knowledge for our global business.\n",
      " \n",
      "FinBERT predicted sentiment:  positive \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for sent in sentences: \n",
    "    tokenized_sent = tokenizer.tokenize(sent)\n",
    "    if len(tokenized_sent) > max_seq_length:\n",
    "        tokenized_sent = tokenized_sent[:max_seq_length]\n",
    "    \n",
    "    ids_review  = tokenizer.convert_tokens_to_ids(tokenized_sent)\n",
    "    mask_input = [1]*len(ids_review)        \n",
    "    padding = [0] * (max_seq_length - len(ids_review))\n",
    "    ids_review += padding\n",
    "    mask_input += padding\n",
    "    input_type = [0]*max_seq_length\n",
    "    \n",
    "    input_ids = torch.tensor(ids_review).to(device).reshape(-1, 256)\n",
    "    attention_mask =  torch.tensor(mask_input).to(device).reshape(-1, 256)\n",
    "    token_type_ids = torch.tensor(input_type).to(device).reshape(-1, 256)\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model(input_ids, token_type_ids, attention_mask)\n",
    "        outputs = F.softmax(outputs,dim=1)\n",
    "        print(sent, '\\nFinBERT predicted sentiment: ', labels[torch.argmax(outputs).item()], '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finbert",
   "language": "python",
   "name": "finbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
