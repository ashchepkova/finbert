{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook contains demo for our fine-tuned Analyst Tone model. We fine-tuned FinBERT model on 10,000 manually annotated analyst statements. You can use this script and infer sentiment on your customerized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-12T03:42:27.110781Z",
     "start_time": "2020-09-12T03:42:27.106997Z"
    }
   },
   "outputs": [],
   "source": [
    "# download pre-trained and fine-tuned weights, unzip to the working directory\n",
    "# https://gohkust-my.sharepoint.com/:u:/g/personal/imyiyang_ust_hk/EQJGiEOkhIlBqlW63TbKA3gBCYgDDcHlBCB7VTXIUMmyiA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.3 (default, Mar 27 2019, 16:54:48) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:03:02.440809Z",
     "start_time": "2020-11-14T03:03:00.771508Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "#from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertConfig\n",
    "from bertModel import BertClassification, dense_opt\n",
    "from datasets import text_dataset, financialPhraseBankDataset\n",
    "import argparse\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:03:02.447859Z",
     "start_time": "2020-11-14T03:03:02.443270Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = {0:'neutral', 1:'positive',2:'negative'}\n",
    "num_labels= len(labels)\n",
    "vocab =\"finance-uncased\"\n",
    "vocab_path = '/Users/svetlana/Downloads/analyst_tone-2/vocab' \n",
    "pretrained_weights_path ='/Users/svetlana/Downloads/analyst_tone-2/pretrained_weights'\n",
    "fine_tuned_weight_path = '/Users/svetlana/Downloads/analyst_tone-2/fine_tuned.pth'   \n",
    "max_seq_length=256\n",
    "device=torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:03:04.572108Z",
     "start_time": "2020-11-14T03:03:02.450447Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/finBERT/from git/bertModel.py:31: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(self.classifier.weight)\n"
     ]
    }
   ],
   "source": [
    "model = BertClassification(weight_path = pretrained_weights_path, \n",
    "                           num_labels=num_labels, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "bert.embeddings.word_embeddings.weight \t torch.Size([30873, 768])\n",
      "bert.embeddings.position_embeddings.weight \t torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight \t torch.Size([2, 768])\n",
      "bert.embeddings.LayerNorm.weight \t torch.Size([768])\n",
      "bert.embeddings.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight \t torch.Size([768, 768])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.encoder.layer.9.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.pooler.dense.weight \t torch.Size([768, 768])\n",
      "bert.pooler.dense.bias \t torch.Size([768])\n",
      "classifier.weight \t torch.Size([3, 768])\n",
      "classifier.bias \t torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:03:10.199945Z",
     "start_time": "2020-11-14T03:03:04.574714Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30873, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(fine_tuned_weight_path, map_location=device))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-12T02:55:51.929515Z",
     "start_time": "2020-09-12T02:55:51.927166Z"
    }
   },
   "source": [
    "# 0 is neutral, 1 is positive, and 2 is negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:03:53.760997Z",
     "start_time": "2020-11-14T03:03:53.755690Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import csv\n",
    "import pandas as pd\n",
    "text_path = '/Users/svetlana/Desktop/Creation/*'\n",
    "files = glob.glob(text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/svetlana/Desktop/Creation/MUR 10102018.txt',\n",
       " '/Users/svetlana/Desktop/Creation/ADM 29102018.txt',\n",
       " '/Users/svetlana/Desktop/Creation/LOW 24092009.txt',\n",
       " '/Users/svetlana/Desktop/Creation/GBX 26102018 .txt',\n",
       " '/Users/svetlana/Desktop/Creation/CME 12012021.txt',\n",
       " '/Users/svetlana/Desktop/Creation/RLGY 15022017.txt',\n",
       " '/Users/svetlana/Desktop/Creation/VTR 06112020.txt',\n",
       " '/Users/svetlana/Desktop/Creation/WMT 11102018.txt',\n",
       " '/Users/svetlana/Desktop/Creation/MDLZ 02072015.txt',\n",
       " '/Users/svetlana/Desktop/Creation/PLD 05112018 .txt']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        #sentence = f.read().strip().split('\\n')\n",
    "        sentence = f.read()\n",
    "    sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EL DORADO, Ark.--(BUSINESS WIRE)--Oct. 10, 2018-- Murphy Oil Corporation (NYSE: MUR) announced today that its wholly owned subsidiary, Murphy Exploration & Production Company - USA, has entered into a definitive agreement to form a new joint venture company with Petrobras America Inc. (“PAI”), a subsidiary of Petrobras (NYSE: PBR). The joint venture company will be comprised of Gulf of Mexico producing assets from Murphy and PAI with Murphy overseeing the operations. The transaction will have an effective date of October 1, 2018 and is expected to close by year-end 2018. \\nBoth companies will contribute all their current producing Gulf of Mexico assets to the joint venture, which will be owned 80 percent by Murphy and 20 percent by PAI. The transaction excludes exploration blocks from both companies, with the exception of PAI’s blocks that hold deep exploration rights. Murphy will pay cash consideration of $900 million to PAI, subject to normal closing adjustments. Additionally, PAI will earn an additional contingent consideration up to $150 million if certain price and production thresholds are exceeded beginning in 2019 through 2025. Also, Murphy will carry $50 million of PAI costs in the St. Malo Field if certain enhanced oil recovery projects are undertaken. Upon closing, Murphy expects to fund the transaction through a combination of cash-on-hand and the company’s senior credit facility. \\nTRANSACTION HIGHLIGHTS \\n• Adds approximately 41,000 net barrels of oil equivalent per day to Murphy’s Gulf of Mexico production, of which 97 percent is oil \\n• Total Murphy Gulf of Mexico production is anticipated to be approximately 60,000 net barrels of oil equivalent per day, post-closing \\n• Provides high-margin production with Gulf Coast prices and expected lease operating expense of approximately $10 to $12 per barrel of oil equivalent \\n• Increases Murphy’s corporate oil-weighted production by approximately nine percentage points to 61 percent, post-closing \\n• Adds approximately 60 million barrels of oil equivalent of Proven (1P) reserves and 86 million barrels of oil equivalent of Proven and Probable (2P) reserves, of which 97% is oil \\n• Allocating a portion of the incremental free cash flow to increase oil-weighted Eagle Ford Shale production \\nMurphy President and Chief Executive Officer Roger W. Jenkins stated, “We are very pleased to partner with Petrobras, a global leader in deep water developments, in our new Gulf of Mexico joint venture. We believe the combined strengths of Petrobras and Murphy will yield significant long-term value for both companies. The addition of high quality, oil-weighted assets, such as the St. Malo Field, complements our existing Gulf of Mexico portfolio. We expect the production from this joint venture to generate meaningful incremental free cash flow that provides us with options for future capital allocation.” \\n\\n',\n",
       " '\\ufeffArcher Daniels Midland Company (NYSE: ADM) and Cargill, Incorporated, have agreed to form a technology joint venture, Grainbridge LLC. The joint venture intends to provide grain marketing decision support, e-commerce and account management software for North American farmers. This includes the development of digital tools designed to help farmers across the U.S. and Canada consolidate information on production economics and grain marketing activities into a single digital platform, at no cost to them.\\n“Farmers can more effectively market grain and improve their profitability using technology and data analytics,” said Roger Watchorn, president of Cargill’s North American agricultural supply chain. “They need digital tools that seamlessly connect their production and cost-of-production data from a variety of sources already used on their farms—but those tools simply aren’t available today. Together, Cargill and ADM’s unique technology capabilities, grain handling operations and years of farm consulting experience allow us to more quickly create the digital tools that will drive efficiency and profitability for farmers.”\\n“Farmers deserve the convenience of technology that gives them the power to make quick, effective and fact-based grain marketing decisions,” said Wes Uhlmeyer, president, ADM Grain. “This new joint venture will provide for these needs with a single, powerful, easy-to-use platform for farmers to manage their accounts, make grain marketing decisions and execute transactions. We’re particularly excited to work with Cargill on this joint venture to offer an unparalleled array of tools that provide farmers across the country access to powerful decision support, business intelligence and transactional efficiency. While remaining strong competitors, we recognize the need to advance technology for farmers and believe that together we can accomplish that common goal.”\\nOne of the first tools being developed by Grainbridge will give farmers secure access to their combined ADM and Cargill transactions, including contracts, scale tickets and payment information. This dashboard is being designed to provide farmers with insights into their current grain position and real-time opportunities.\\nSoon after, the new venture intends to introduce a simple and easy way for farmers to execute grain transactions on their own schedule. With this platform, farmers will have access to automated intelligence that illustrates changing break-even levels (updated based on current crop conditions), margin price targets, total current grain and risk management positions, revenue at risk and access to benchmark and historical insights.\\nGrainbridge welcomes other grain companies, grain buyers, technology and data providers to participate on the platform. This open approach will enhance the information available on the platform, giving farmers a complete, seamless 360-degree view of their business without having to consolidate these information sources on their own. Strong protections built into the system will safeguard customer data and prioritize privacy. Data will not be on the Grainbridge platform unless approved by the individual farmer, and individual customer data will not be shared between participating companies.\\n“We are often asked by our farmer customers to provide technology solutions that provide them relief to a problem they are experiencing—and we can sense the underlying need to solve a variety of problems that go unmentioned,” said Uhlmeyer. “This new platform will be a tool they’ve long needed that solves many of those known and suspected pain points we hear about each day. We’re excited to bring these types of innovative and progressive solutions to the industry.”\\n“The ultimate goal is to help farmers make better, smarter decisions, using the data that is already existing in their operations. We are simply putting that data and related insights at their fingertips, allowing them to more effectively adapt to changing market conditions,” said Watchorn. “Both Cargill and ADM are committed to helping farmers succeed. It’s why we’re in this business, and why we’ll continue to invest in these tools well into the future.”\\nThe transaction is subject to regulatory approval and is expected to close in the next few months. Terms of the joint venture are not being disclosed.\\n\\n',\n",
       " 'MOORESVILLE, N.C., Aug. 24 /PRNewswire-FirstCall/ -- Lowe\\'s Companies, Inc. (NYSE: LOW) announced today the company has entered into a joint venture agreement with Australia\\'s largest retailer, Woolworths Limited (ASX: WOW), to develop a network of home improvement stores for consumers in Australia. Under the agreement, Lowe\\'s will be one-third owner of the destination home improvement chain, which is slated to open its first store in the 2011 fiscal year.\\n\"This is a tremendous opportunity to enter the A$24 billion ($20 billion USD) and growing home improvement market in Australia, at a time when the sector is underserved,\" explained Robert A. Niblock, Lowe\\'s chairman and CEO. \"By partnering Lowe\\'s home improvement knowledge with the retail expertise of Australia\\'s largest retailer, Woolworths, we expect to develop destination home improvement stores that offer consumers the products and services they need for their homes and businesses in an environment that is comfortable and easy to shop.\"\\nWoolworths, founded in 1924, is the 23rd( )largest retailer in the world with expertise across grocery, consumer electronics, liquor, petroleum and general merchandise retailing. With 2009 fiscal year revenues of A$49.4 billion ($37 billion USD), the company has more than 3,000 retail outlets and is the largest private sector employer in Australia. \\n\"The Australian love of property and high levels of home ownership mean that maintaining and improving homes is an important part of everyday life,\" said Michael Luscombe, Woolworths chief executive. \"We\\'re very pleased to be working with Lowe\\'s, a leading global retailer in home improvement, and very proud of the unique and collaborative agreement we have put in place.\" \\nLowe\\'s and Woolworths plan to share best practices to create a leading destination home improvement business in Australia. The partners have a target to secure more than 150 store sites over the next five years, with the first store expected to open in late 2011. The locations will be large-format stores, greater than 100,000 square feet, and will offer products and services for both the do-it-yourself homeowner and the trade customer. Already, Woolworths has secured entitlement to 12 sites and is currently in final negotiations to secure a further 15 sites for Greenfield development.\\nIn addition, the joint venture has made a recommended takeover offer for all the shares in Danks Holdings Limited (ASX: DKS), Australia\\'s second largest hardware distributor. \\n\\n',\n",
       " '\\ufeffLAKE OSWEGO, Ore., Oct. 26, 2018 /PRNewswire/ --\\xa0The Greenbrier Companies, Inc. (NYSE:GBX) and Saudi Railway Company (\"SAR\") announced today that they have signed an agreement under which the parties will invest and generate investments totaling 1 billion Saudi riyals (USD $270 million) in the Saudi rail industry.\\xa0 Greenbrier and SAR intend to establish a joint venture company (\"JV\") in Saudi Arabia to execute railway projects and supply railcars for profitable growth of the Saudi freight rail market.\\xa0 \\n\\nTogether through the JV, Greenbrier and SAR will establish a new multi-modal business centered on creating and maximizing existing and new rail routes for freight movement throughout the Kingdom and, ultimately, the Gulf Cooperation Council (GCC) region.\\xa0 The JV will invest in assets and infrastructure required to expand profitable rail service offerings to the Saudi market. \\xa0Based on achieving identified milestones, Greenbrier will provide the JV up to $100 million USD (370 million Saudi riyals) in new railcars, lift equipment and other terminal investment necessary to place railcars in revenue service, and will operate intermodal and other freight terminals. SAR will provide the JV locomotives, rail access and service schedules to facilitate line haul services.\\xa0 Using its investment syndication model, Greenbrier will facilitate raising an additional $170 million USD (630 million Saudi riyals) in collaboration with SAR and international public and private investment communities. The JV will operate similar to the model of TTX in North America on car supply, and can expand to serve GCC nations. Greenbrier will have a first right to manufacture and provide railcars for the JV\\'s railcar pool and establish a Saudi-based manufacturing/assembly presence.\\xa0 \\nPooling brings several benefits to SAR including technology and knowledge transfer, advanced railcar designs and efficient network service design.\\xa0 The JV will promote supply chain efficiencies by increasing local content, reducing the capital burden on SAR, lowering SAR\\'s operating costs and improving the railroad\\'s competitiveness with highway transport.\\xa0 Freight traffic in Saudi Arabia is predominantly transported by truck or by pipeline. The JV will help SAR meet its customers\\' needs by providing well-maintained railcars in an efficient, network system for general freight traffic.\\xa0\\xa0\\xa0 \\nThe JV is consistent with and will serve the Kingdom\\'s Vision 2030 National Priorities, the National Transformation Program Strategic Objectives and the National Industrial Clusters Development Program adopted by the Saudi Arabian government.\\xa0 Among other goals, the objectives of the JV include: (i) accelerating the development of Saudi Arabia into a regional logistics center by fully integrating rail into the freight transport sector; (ii) maximizing in-country employment opportunities in railcar manufacturing, railcar asset support services and supply chain logistics related to the Saudi rail system; and (iii) increasing safety and improving the livability of Saudi cities.\\xa0 The JV is intended to create a self-funding business, following the North American railroad model, which will reduce reliance on government funding for the rail sector over time and enhance value.\\nWilliam A. Furman, Chairman, CEO and President of Greenbrier said, \"The JV will benefit the people of Saudi Arabia by enhancing the capacity and efficiency of freight and logistics systems in the Kingdom while contributing to local job growth, economic development and national defense mobility.\\xa0 As the Kingdom advances economic diversification to improve the life of its people, it will significantly grow its transportation infrastructure.\\xa0 We are pleased to partner with SAR as it addresses these needs in its rail network.\\xa0 We view this as a core part of our mission in every geography where we operate.\"\\xa0 \\nThe relationship between SAR and Greenbrier began in 2015 when Greenbrier was awarded a contract with the Public Investment Fund (PIF) to manufacture nearly 1,200 tank wagons for SAR to use in transporting molten Sulphur and phosphoric acid.\\xa0 These are commodities that the National Mining Company (Ma\\'aden) uses to produce fertilizer as part of the Wa\\'ad Al-Shammal (\"Northern Promise\") development project.\\xa0 Saudi Arabia and its people have been long-term friends and allies of America.\\xa0 This JV is the culmination of more than a year of collaboration between Greenbrier and SAR accompanied by regular consultation with our respective government representatives.\\nThe JV is subject to the completion of final due diligence by the parties and required government or corporate approvals.\\n\\n',\n",
       " \"\\ufeffCME Group (NASDAQ: CME), the world's leading and most diverse derivatives marketplace, and IHS Markit (NYSE: INFO), a world leader in critical information, analytics and solutions, today announced that they have agreed to combine their post-trade services into a new joint venture.\\nThe new company will include trade processing and risk mitigation operations. It will incorporate CME Group’s optimization businesses – Traiana, TriOptima and Reset – and IHS Markit’s MarkitSERV.\\nThe combination of these complementary offerings will provide clients with enhanced platforms and services for global OTC markets across interest rate, FX, equity and credit asset classes. Market participants will benefit from a more efficient front-to-back workflow with enhanced connectivity and improved trading certainty. As a result, OTC market participants will be able to improve risk management and streamline post-trade operations.\\n“As OTC derivatives markets become increasingly fragmented, market participants are seeking to more effectively manage their risk and optimize their balance sheets,” said Terry Duffy, CME Group Chairman and Chief Executive Officer. “By combining the strengths of these diverse businesses into one organization, the joint venture will be better positioned to serve clients worldwide by driving innovation, developing valuable products and providing operational efficiencies.”\\n“As the trade processing ecosystem continues to evolve, we believe there is an opportunity to reduce operational complexity and deliver additional value to customers,” said Lance Uggla, IHS Markit Chairman and Chief Executive Officer. “Through our combined resources and best-of-breed services, we will have a strengthened framework to serve dynamic global markets and design new solutions in partnership with our joint customer base.”\\nTraiana, TriOptima and Reset offer a wide range of pre- and post-trade services with capital efficiencies across markets, delivering trade processing connectivity, credit controls, optimization and risk mitigation solutions.\\nMarkitSERV provides end-to-end trade processing and workflow solutions that support all participants across the derivatives and FX markets, from post-trade notices of execution, trade confirmation and allocations to clearing and reporting.\\nThe transaction is expected to close in summer 2021 subject to customary antitrust and regulatory approvals and other customary closing conditions.\\nAt closing, IHS Markit will make an equalization payment of $113 million to CME Group to achieve 50/50 ownership and shared control in the joint venture. Further financial terms were not disclosed.\\n\\n\",\n",
       " 'MADISON, N.J.\\xa0and CHICAGO, Feb. 15, 2017 /PRNewswire/ -- Realogy Holdings Corp. (NYSE: RLGY), the largest full-service residential real estate services company in the United States, and Guaranteed Rate, Inc. (\"Guaranteed Rate\"), one of the largest independent retail mortgage companies in the United States, today announced that they have agreed to form a new joint venture, Guaranteed Rate Affinity LLC, which is expected to begin doing business in June 2017. Commencement of operations is subject to the closing of an asset purchase agreement under which Guaranteed Rate Affinity will acquire certain assets of the mortgage operations of PHH Home Loans LLC, the existing joint venture between Realogy and PHH Mortgage, including its four regional mortgage origination and processing centers, its relocation division and employees across the United States.\\nGuaranteed Rate Affinity will originate and market its mortgage lending services to Realogy\\'s real estate brokerage and relocation subsidiaries, respectively NRT and Cartus, as well as to other real estate brokerage and relocation companies across the country. Guaranteed Rate Affinity also will market its mortgage lending services to a broad consumer audience while leveraging its end-to-end online platform to drive growth in those markets. Guaranteed Rate will own a controlling 50.1% stake of Guaranteed Rate Affinity and Realogy will own 49.9%. \\n\"This is a unique opportunity for us to accelerate our growth on a national level by bringing our cutting edge technology together with Realogy, a Fortune 500 company,\" said Guaranteed Rate Chief Executive Officer and Founder Victor Ciardelli. \"This new partnership aligns deeply with our core values, especially our commitment to \\'Work with the Best of the Best.\\' We appreciate the hard work and commitment of the current joint venture employees during this transition period and we look forward to having them become part of the Guaranteed Rate Affinity team.\"\\n\"As we evaluated potential new options for our mortgage origination venture, Guaranteed Rate was clearly the right strategic partner to help our company-owned brokerage business and its affiliated sales associates offer an innovative and streamlined mortgage process built on best-in-class technology,\" said Richard A. Smith, Realogy\\'s Chairman, Chief Executive Officer and President. \"Mortgage financing is a service we have provided for more than 20 years. We are delighted to partner with Guaranteed Rate and are excited to embark on this new relationship that will substantially enhance our service offerings to our customers.\"\\n\"This joint venture is a positive development that helps to elevate NRT\\'s overall agent value proposition,\" said Bruce Zipf, Chief Executive Officer and President of NRT, the Realogy subsidiary that owns and operates residential brokerage companies with approximately 800 offices and 47,500 independent sales associates across the United States.\\xa0\"Guaranteed Rate has built one of the market\\'s most trusted brands through seamless technology and outstanding customer service. We look forward to working with them and allowing our independent sales associates to offer their clients access to an innovative online experience.\"\\nThe asset purchase agreement is subject to approval by PHH Corporation shareholders and certain other closing conditions and is expected to be completed in a series of asset sale closings. The initial closing is expected to occur in June 2017, and the final closing is expected to occur during the fourth quarter of 2017.\\n\\n',\n",
       " '\\ufeffCHICAGO--(BUSINESS WIRE)--Ventas, Inc. (NYSE: VTR) (“Ventas” or the “Company”), an S&P 500 company and one of the world’s leading owners of healthcare, senior housing and research & innovation properties, announced today it has formed a joint venture (the “JV”) with GIC. The JV will initially own four in-progress university-based Research & Innovation (“R&I”) development projects (the “Initial R&I JV Projects”) with total project costs estimated at $930 million. The JV may be expanded to include other pre-identified future R&I development projects.\\n“We are excited to announce this attractive R&I development partnership with GIC, one of the world’s most respected real estate investors,” said Debra A. Cafaro, Ventas Chairman & CEO. “With this strategic partnership, we continue to diversify our capital sources, retain a majority interest in our ongoing R&I developments, accelerate additional projects from our pipeline of opportunities and enhance our liquidity and financial flexibility. With GIC and Wexford, we look forward to successfully completing our in-progress R&I projects and launching additional R&I developments from our existing pipeline together.”\\n“Following the successful launch and continued growth of our perpetual life vehicle focused on investing in core healthcare real estate, we are pleased to further expand our third-party capital management platform through this JV to now have over $3 billion in assets under management. Our success demonstrates the strength of Ventas’s capabilities, relationships and team in the healthcare real estate market. We are confident in our ability to extend our track record of value creation.”\\nLee Kok Sun, Chief Investment Officer of Real Estate, GIC said, “As a long-term investor, we are confident that the life sciences sector will continue to flourish, driven by the growing and aging global population, as well as increased public and private funding for life sciences R&D. We are pleased to establish this partnership with Ventas, and look forward to scaling our venture together.”\\nTransaction Highlights\\n• Ventas contributed its ownership interest in the Initial R&I JV Projects into the JV. Ventas will own an over 50 percent interest, and GIC will own a 45 percent interest, in the Initial R&I JV Projects. Ventas’s exclusive development partner, Wexford Science & Technology (“Wexford”), remains the developer of, and a minority partner in, all of the Initial R&I JV Projects.\\n• Ventas will act as manager of the JV, with customary rights and obligations, and will receive customary fees and incentives.\\n• Total costs for the Initial R&I JV Projects are expected to approximate $930 million. The Initial R&I JV Projects will contain 1.4 million square feet when completed and are approximately 65 percent pre-leased in aggregate. They are expected to open between 2021 and 2023 and include:\\n• One uCity:\\xa0Expansion of the Philadelphia uCity Square submarket associated with the University of Pennsylvania. Construction on this project recommenced on October 1, 2020 after having been paused as part of Ventas’s proactive capital conservation actions in response to COVID-19.\\n• College of Nursing and Health Professions (“CNHP”), Drexel University:\\xa0State-of-the-art academic medicine facility, also in uCity Square, which will provide CNHP students, faculty and staff with immediate access to Drexel’s full suite of on-campus resources.\\n• Pitt Immune Transplant & Therapy Center (Phases I & II):\\xa0Creation of a research, academic medicine and innovation hub anchored by a new relationship with the University of Pittsburgh to house cutting-edge immunotherapy research in collaboration with the University of Pittsburgh Medical Center (“UPMC”) and co-located with UPMC’s Shadyside Hospital.\\n• Arizona State University:\\xa0Class-A, fully lab-enabled R&I center anchored by Arizona State University and focused on biomedical discovery and innovation in health outcomes.\\n• Total project costs of approximately $180 million have been incurred to-date on the Initial R&I JV Projects, which are in various stages of development and construction. At closing, GIC reimbursed Ventas for its share of costs incurred to-date.\\n• Ventas and GIC will each contribute its pro rata share of the future costs to complete the Initial R&I JV Projects. The Initial R&I JV Projects are expected to be financed with approximately $500 million in construction financing.\\n• Ventas and GIC have the opportunity to add certain additional pre-identified R&I development projects currently in the Company’s pipeline to the JV over time (the “Pipeline JV Projects”). The Pipeline JV Projects, if all completed, would expand the JV to encompass over $2 billion in total expected project costs. Wexford is also the developer of, and would be a minority partner in, the Pipeline JV Projects.\\nFinancial and Strategic Benefits\\xa0\\n• The JV diversifies Ventas’s capital sources via a new, strategic partnership with GIC, a leading, global real estate investor with significant office, life science and healthcare expertise.\\n• Ventas maintains a majority interest in Initial R&I JV Projects and its R&I development pipeline.\\n• Ventas’s enterprise liquidity and financial profile are enhanced through GIC’s responsibility for its share of already incurred and future development costs of the Initial R&I JV Projects.\\n• The JV demonstrates the strength of Ventas’s capabilities, platform and team and represents an expansion of its third-party capital management business. This partnership builds on Ventas’s positive momentum and access to private capital, following the successful launch and growth of the Ventas Life Science and Healthcare Real Estate Fund including its recent acquisition of a $1 billion portfolio of trophy life sciences properties in South San Francisco.\\n\\n',\n",
       " 'BENTONVILLE, Ark., Oct. 11, 2018 – Today, Walmart Inc. announced a strategic entertainment joint venture with Eko, a developer of interactive video technology. The joint venture includes plans to develop original, interactive content that will enable Walmart to connect with customers in new and more meaningful ways, with the goal of driving deeper and more frequent engagement.\\nThe content, which could include a range of offerings, from cooking shows to interactive toy catalogues, will go beyond the basic personalization available today, allowing viewers to participate in and shape stories as they are being told. The result will be an experience unique to each participant, creating more engaged and emotionally-connected audiences.\\nThe joint venture expands Walmart’s entertainment ecosystem. The retailer already has a strong physical and digital video presence, through stores, websites, the digital platform VUDU and the recently launched eBook platform, Walmart eBooks, with Rakuten Kobo. This joint venture strengthens Walmart’s continued presence in the evolving entertainment landscape. \\nOur partnership with Eko will help us accelerate efforts to deepen relationships with customers and connect with new audiences in innovative ways and is one part of an overall entertainment ecosystem we’re building. By partnering with organizations across the industry to create original, interactive content, we’re bringing the next generation of entertainment to customers and delivering memorable experiences they can only find at Walmart.\\n- Scott McCall, senior vice president for entertainment, toys and seasonal, Walmart U.S. \\nSince 2010, Eko has pioneered the future of entertainment, alongside partners like Sony Pictures Entertainment and MGM Studios. Eko has received prior funding from Sequoia Capital, Intel Capital, Warner Music Group, Samsung, Walmart, and others, and has more than 15 patents for its technology. In connection with the joint venture, Walmart has agreed to participate in Eko’s next funding round.\\n“The future of video entertainment is interactive, and this joint venture is a huge step towards bringing this future to life,” said Yoni Bloch, chief executive officer of Eko. “In 2018, all forms of media are personalized except for live action video. At Eko, our mission is to evolve past basic personalization and partnering with Walmart will accelerate that evolution. We’re working alongside some of the most creative people from Hollywood and around the world, and we invite others to join us in making great interactive content.”\\nThe joint venture, known as W*E Interactive Ventures, will be led by Bloch, with input from several industry experts. Tribeca Productions co-founder Jane Rosenthal will serve as strategic advisor. Rosenthal, producer of films such as Meet the Parents, Meet the Fockers, About a Boy (film and series), Wag the Dog and the upcoming Scorsese film The Irishman, helps create a bridge for both established and emerging artists looking for an outlet for their work. Nancy Tellem, chief media officer and executive chairwoman of Eko, will serve on the board of the joint venture. Prior to joining Eko, Tellem spent more than 25 years in television including time as president of CBS Network Television Group.\\n\"During my career in broadcast television, I\\'ve seen how traditional media has been transformed by technology, and have long believed that technology would be the key to creating more engaging entertainment experiences,\" said Nancy Tellem, executive chairman and chief media officer of Eko. \"Audiences are hungry for immersive entertainment and storytellers are embracing this new technology in creating a new type of story narrative that deeply engages the viewer. Now is the time for Eko and interactive content to take center stage.\"\\n\\n',\n",
       " \"Shanghai, July 2, 2015 – Mondelēz International, Inc. (NASDAQ: MDLZ) and D.E Master Blenders 1753 B.V. announced today that they have completed the transactions to combine their respective coffee businesses, including Mondelēz International’s coffee portfolio in France, to create JACOBS DOUWE EGBERTS (JDE), which will be the world’s leading pure-play coffee company with annual revenues of more than €5 billion.\\n\\nUpon closing, Mondelēz International received cash of approximately €3.8 billion and 44 percent interest in the new joint venture, subject to standard post-close adjustments. Acorn Holdings B.V. (AHBV), owner of D.E Master Blenders 1753, will have a 56 percent share in JDE. AHBV is owned by an investor group led by JAB Holding Company in partnership with BDT Capital Partners, Quadrant Capital Advisors and Société Familiale d'Investissements.\\n\\nFollowing the transactions, Mondelēz International becomes an even more focused snacking company, with approximately 85 percent of net revenues derived from biscuits, chocolate, gum and candy. By retaining a significant stake in JDE, however, Mondelēz International will continue to benefit from future growth of the coffee category.\",\n",
       " 'Nov 05, 2018 \\nSAN FRANCISCO, Nov. 5, 2018 /PRNewswire/ --\\xa0Prologis, Inc. (NYSE: PLD), the global leader in logistics real estate, and global real estate investor Ivanhoé Cambridge today announced an agreement to form a 20/80 joint venture in Brazil. \\nPrologis Brazil Logistics Venture will develop and operate logistics real estate in Brazil, specifically in São Paulo and Rio de Janeiro. At closing, the venture will acquire an initial portfolio of assets of approximately 6.9 million square feet of operating properties and 371 acres of land from Prologis\\' balance sheet with a commitment to build out the existing land bank.\\n\"We are very excited to continue building our highly successful Brazil business with Ivanhoé Cambridge, a new strategic partner that brings extensive experience in Brazil specifically and in global real estate generally,\" said Gene Reilly, Prologis\\' CEO of the Americas. \"With this venture, we are increasingly well-positioned to meet the growing demand for modern logistics space – as more than 40 percent of the country\\'s GDP is located in São Paulo and Rio de Janeiro.\"\\nRita-Rose Gagné, President, Growth Markets for Ivanhoé Cambridge added: \"We have been investing in Brazil since 2006 and continue to do so with a long-term view. The quality of our partners and assets are key. We are delighted to partner with Prologis, the leading global logistics company, to invest in the growing logistics sector of Brazil. This venture aligns with our global approach to significantly increase our investments in the industrial and logistics space in key markets around the world.\"\\xa0 \\nThe transaction is subject to customary closing conditions, including applicable regulatory and third-party approvals.\\n\\n']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EL DORADO, Ark.--(BUSINESS WIRE)--Oct. 10, 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>﻿Archer Daniels Midland Company (NYSE: ADM) an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MOORESVILLE, N.C., Aug. 24 /PRNewswire-FirstCa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>﻿LAKE OSWEGO, Ore., Oct. 26, 2018 /PRNewswire/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>﻿CME Group (NASDAQ: CME), the world's leading ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MADISON, N.J. and CHICAGO, Feb. 15, 2017 /PRNe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>﻿CHICAGO--(BUSINESS WIRE)--Ventas, Inc. (NYSE:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BENTONVILLE, Ark., Oct. 11, 2018 – Today, Walm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shanghai, July 2, 2015 – Mondelēz Internationa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Nov 05, 2018 \\nSAN FRANCISCO, Nov. 5, 2018 /PR...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  EL DORADO, Ark.--(BUSINESS WIRE)--Oct. 10, 201...\n",
       "1  ﻿Archer Daniels Midland Company (NYSE: ADM) an...\n",
       "2  MOORESVILLE, N.C., Aug. 24 /PRNewswire-FirstCa...\n",
       "3  ﻿LAKE OSWEGO, Ore., Oct. 26, 2018 /PRNewswire/...\n",
       "4  ﻿CME Group (NASDAQ: CME), the world's leading ...\n",
       "5  MADISON, N.J. and CHICAGO, Feb. 15, 2017 /PRNe...\n",
       "6  ﻿CHICAGO--(BUSINESS WIRE)--Ventas, Inc. (NYSE:...\n",
       "7  BENTONVILLE, Ark., Oct. 11, 2018 – Today, Walm...\n",
       "8  Shanghai, July 2, 2015 – Mondelēz Internationa...\n",
       "9  Nov 05, 2018 \\nSAN FRANCISCO, Nov. 5, 2018 /PR..."
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(sentences)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffCHICAGO--(BUSINESS WIRE)--Ventas, Inc. (NYSE: VTR) (“Ventas” or the “Company”), an S&P 500 company and one of the world’s leading owners of healthcare, senior housing and research & innovation properties, announced today it has formed a joint venture (the “JV”) with GIC. The JV will initially own four in-progress university-based Research & Innovation (“R&I”) development projects (the “Initial R&I JV Projects”) with total project costs estimated at $930 million. The JV may be expanded to include other pre-identified future R&I development projects.\\n“We are excited to announce this attractive R&I development partnership with GIC, one of the world’s most respected real estate investors,” said Debra A. Cafaro, Ventas Chairman & CEO. “With this strategic partnership, we continue to diversify our capital sources, retain a majority interest in our ongoing R&I developments, accelerate additional projects from our pipeline of opportunities and enhance our liquidity and financial flexibility. With GIC and Wexford, we look forward to successfully completing our in-progress R&I projects and launching additional R&I developments from our existing pipeline together.”\\n“Following the successful launch and continued growth of our perpetual life vehicle focused on investing in core healthcare real estate, we are pleased to further expand our third-party capital management platform through this JV to now have over $3 billion in assets under management. Our success demonstrates the strength of Ventas’s capabilities, relationships and team in the healthcare real estate market. We are confident in our ability to extend our track record of value creation.”\\nLee Kok Sun, Chief Investment Officer of Real Estate, GIC said, “As a long-term investor, we are confident that the life sciences sector will continue to flourish, driven by the growing and aging global population, as well as increased public and private funding for life sciences R&D. We are pleased to establish this partnership with Ventas, and look forward to scaling our venture together.”\\nTransaction Highlights\\n• Ventas contributed its ownership interest in the Initial R&I JV Projects into the JV. Ventas will own an over 50 percent interest, and GIC will own a 45 percent interest, in the Initial R&I JV Projects. Ventas’s exclusive development partner, Wexford Science & Technology (“Wexford”), remains the developer of, and a minority partner in, all of the Initial R&I JV Projects.\\n• Ventas will act as manager of the JV, with customary rights and obligations, and will receive customary fees and incentives.\\n• Total costs for the Initial R&I JV Projects are expected to approximate $930 million. The Initial R&I JV Projects will contain 1.4 million square feet when completed and are approximately 65 percent pre-leased in aggregate. They are expected to open between 2021 and 2023 and include:\\n• One uCity:\\xa0Expansion of the Philadelphia uCity Square submarket associated with the University of Pennsylvania. Construction on this project recommenced on October 1, 2020 after having been paused as part of Ventas’s proactive capital conservation actions in response to COVID-19.\\n• College of Nursing and Health Professions (“CNHP”), Drexel University:\\xa0State-of-the-art academic medicine facility, also in uCity Square, which will provide CNHP students, faculty and staff with immediate access to Drexel’s full suite of on-campus resources.\\n• Pitt Immune Transplant & Therapy Center (Phases I & II):\\xa0Creation of a research, academic medicine and innovation hub anchored by a new relationship with the University of Pittsburgh to house cutting-edge immunotherapy research in collaboration with the University of Pittsburgh Medical Center (“UPMC”) and co-located with UPMC’s Shadyside Hospital.\\n• Arizona State University:\\xa0Class-A, fully lab-enabled R&I center anchored by Arizona State University and focused on biomedical discovery and innovation in health outcomes.\\n• Total project costs of approximately $180 million have been incurred to-date on the Initial R&I JV Projects, which are in various stages of development and construction. At closing, GIC reimbursed Ventas for its share of costs incurred to-date.\\n• Ventas and GIC will each contribute its pro rata share of the future costs to complete the Initial R&I JV Projects. The Initial R&I JV Projects are expected to be financed with approximately $500 million in construction financing.\\n• Ventas and GIC have the opportunity to add certain additional pre-identified R&I development projects currently in the Company’s pipeline to the JV over time (the “Pipeline JV Projects”). The Pipeline JV Projects, if all completed, would expand the JV to encompass over $2 billion in total expected project costs. Wexford is also the developer of, and would be a minority partner in, the Pipeline JV Projects.\\nFinancial and Strategic Benefits\\xa0\\n• The JV diversifies Ventas’s capital sources via a new, strategic partnership with GIC, a leading, global real estate investor with significant office, life science and healthcare expertise.\\n• Ventas maintains a majority interest in Initial R&I JV Projects and its R&I development pipeline.\\n• Ventas’s enterprise liquidity and financial profile are enhanced through GIC’s responsibility for its share of already incurred and future development costs of the Initial R&I JV Projects.\\n• The JV demonstrates the strength of Ventas’s capabilities, platform and team and represents an expansion of its third-party capital management business. This partnership builds on Ventas’s positive momentum and access to private capital, following the successful launch and growth of the Ventas Life Science and Healthcare Real Estate Fund including its recent acquisition of a $1 billion portfolio of trophy life sciences properties in South San Francisco.\\n\\n'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.at[6,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:03:54.277505Z",
     "start_time": "2020-11-14T03:03:54.217928Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer(vocab_file = vocab_path, do_lower_case = True, \n",
    "                          do_basic_tokenize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2179: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "for sent in sentences:\n",
    "    encoded_data = tokenizer.batch_encode_plus(\n",
    "    sentences, add_special_tokens = True,\n",
    "    return_attention_mask = True,\n",
    "    max_length = max_seq_length,\n",
    "    pad_to_max_length = True,\n",
    "    return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.empty(len(sentences), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "input_ids = encoded_data['input_ids']\n",
    "attention_masks = encoded_data['attention_mask']\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(input_ids, attention_masks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    sampler = SequentialSampler(dataset),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataloader_):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for batch in dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels': batch[2],\n",
    "                 }\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        outputs = F.softmax(outputs,dim=1)\n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        \n",
    "        predictions.append(outputs)\n",
    "        \n",
    "    predictions = np.concatenate(predictions, axis = 0)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99961257e-01, 3.83020051e-05, 5.08418395e-07],\n",
       "       [2.75445014e-01, 7.24547803e-01, 7.13355075e-06],\n",
       "       [1.51895314e-01, 8.48099291e-01, 5.35283652e-06],\n",
       "       [9.35520649e-01, 6.44244477e-02, 5.49040778e-05],\n",
       "       [5.07179209e-07, 9.99999523e-01, 1.42591778e-08],\n",
       "       [9.99375403e-01, 6.07218943e-04, 1.74464185e-05],\n",
       "       [2.20124647e-01, 7.79869616e-01, 5.77074843e-06],\n",
       "       [2.54127372e-05, 9.99974489e-01, 1.09009946e-07],\n",
       "       [9.99278605e-01, 7.19159783e-04, 2.21359687e-06],\n",
       "       [7.80923432e-03, 9.92190063e-01, 7.44203078e-07]], dtype=float32)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = predict(dataloader_ = dataloader)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 0 4\n",
      "0.6 0.0 0.4\n"
     ]
    }
   ],
   "source": [
    "positives = 0\n",
    "negatives = 0\n",
    "neutrals = 0\n",
    "sentiment = []\n",
    "for i in range(len(pred)):\n",
    "    pred_labels_i = np.argmax(pred[i]).flatten()\n",
    "    sentiment.append(pred_labels_i[0])\n",
    "    if pred_labels_i[0] == 1:\n",
    "        positives+=1\n",
    "    elif pred_labels_i[0] == 2:\n",
    "        negatives+=1\n",
    "    elif pred_labels_i[0] == 0:\n",
    "        neutrals+=1\n",
    "print(positives, negatives, neutrals) \n",
    "print(positives / len(pred), negatives / len(pred), neutrals / len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 0, 1, 0, 1, 1, 0, 1]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EL DORADO, Ark.--(BUSINESS WIRE)--Oct. 10, 201...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>﻿Archer Daniels Midland Company (NYSE: ADM) an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MOORESVILLE, N.C., Aug. 24 /PRNewswire-FirstCa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>﻿LAKE OSWEGO, Ore., Oct. 26, 2018 /PRNewswire/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>﻿CME Group (NASDAQ: CME), the world's leading ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MADISON, N.J. and CHICAGO, Feb. 15, 2017 /PRNe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>﻿CHICAGO--(BUSINESS WIRE)--Ventas, Inc. (NYSE:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BENTONVILLE, Ark., Oct. 11, 2018 – Today, Walm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shanghai, July 2, 2015 – Mondelēz Internationa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Nov 05, 2018 \\nSAN FRANCISCO, Nov. 5, 2018 /PR...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  sentiment\n",
       "0  EL DORADO, Ark.--(BUSINESS WIRE)--Oct. 10, 201...          0\n",
       "1  ﻿Archer Daniels Midland Company (NYSE: ADM) an...          1\n",
       "2  MOORESVILLE, N.C., Aug. 24 /PRNewswire-FirstCa...          1\n",
       "3  ﻿LAKE OSWEGO, Ore., Oct. 26, 2018 /PRNewswire/...          0\n",
       "4  ﻿CME Group (NASDAQ: CME), the world's leading ...          1\n",
       "5  MADISON, N.J. and CHICAGO, Feb. 15, 2017 /PRNe...          0\n",
       "6  ﻿CHICAGO--(BUSINESS WIRE)--Ventas, Inc. (NYSE:...          1\n",
       "7  BENTONVILLE, Ark., Oct. 11, 2018 – Today, Walm...          1\n",
       "8  Shanghai, July 2, 2015 – Mondelēz Internationa...          0\n",
       "9  Nov 05, 2018 \\nSAN FRANCISCO, Nov. 5, 2018 /PR...          1"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "groups = defaultdict(list)\n",
    "roots = []\n",
    "tickers = []\n",
    "dates = []\n",
    "for file in files:\n",
    "    basename = os.path.basename(file)\n",
    "    root, extension = os.path.splitext(basename)\n",
    "    roots.append(root)\n",
    "    for root in roots:\n",
    "        ticker = str(root).split()[0]\n",
    "        date = str(root).split()[1]\n",
    "    tickers.append(ticker)\n",
    "    dates.append(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ticker'] = tickers\n",
    "df['dates'] = dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['dates'], format=\"%d%m%Y\", infer_datetime_format=True)\n",
    "df = df.drop(columns = ['dates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yahoo_fin import stock_info as si\n",
    "from yahoo_fin.stock_info import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018-10-10    31.770000\n",
       "2018-10-11    35.459999\n",
       "2018-10-12    35.779999\n",
       "2018-10-15    35.560001\n",
       "2018-10-16    35.380001\n",
       "                ...    \n",
       "2021-02-26    16.330000\n",
       "2021-03-01    16.780001\n",
       "2021-03-02    16.510000\n",
       "2021-03-03    17.459999\n",
       "2021-03-04    18.830000\n",
       "Name: close, Length: 603, dtype: float64"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_data = si.get_data(ticker = 'MUR', start_date = '2018-10-10')['close']\n",
    "price_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data_list = []\n",
    "for ticker, date in zip(df.ticker, df.date): \n",
    "    price_data = si.get_data(ticker, start_date = date, \n",
    "                             end_date = date + timedelta.Timedelta(days=1))['close']\n",
    "    price_data_list.append(price_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['31.77', '47.009998', '21.389999', '48.810001', '201.490005', '27.290001', '39.639999', '93.919998', '41.549999', '65.07']\n"
     ]
    }
   ],
   "source": [
    "price = []\n",
    "price_change = []\n",
    "for i in price_data_list:\n",
    "    price_only = str(i).split()[1]\n",
    "    price.append(price_only)\n",
    "    \n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'] = pd.DataFrame(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EL DORADO, Ark.--(BUSINESS WIRE)--Oct. 10, 201...</td>\n",
       "      <td>0</td>\n",
       "      <td>MUR</td>\n",
       "      <td>2018-10-10</td>\n",
       "      <td>31.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>﻿Archer Daniels Midland Company (NYSE: ADM) an...</td>\n",
       "      <td>1</td>\n",
       "      <td>ADM</td>\n",
       "      <td>2018-10-29</td>\n",
       "      <td>47.009998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MOORESVILLE, N.C., Aug. 24 /PRNewswire-FirstCa...</td>\n",
       "      <td>1</td>\n",
       "      <td>LOW</td>\n",
       "      <td>2009-09-24</td>\n",
       "      <td>21.389999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>﻿LAKE OSWEGO, Ore., Oct. 26, 2018 /PRNewswire/...</td>\n",
       "      <td>0</td>\n",
       "      <td>GBX</td>\n",
       "      <td>2018-10-26</td>\n",
       "      <td>48.810001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>﻿CME Group (NASDAQ: CME), the world's leading ...</td>\n",
       "      <td>1</td>\n",
       "      <td>CME</td>\n",
       "      <td>2021-01-12</td>\n",
       "      <td>201.490005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MADISON, N.J. and CHICAGO, Feb. 15, 2017 /PRNe...</td>\n",
       "      <td>0</td>\n",
       "      <td>RLGY</td>\n",
       "      <td>2017-02-15</td>\n",
       "      <td>27.290001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>﻿CHICAGO--(BUSINESS WIRE)--Ventas, Inc. (NYSE:...</td>\n",
       "      <td>1</td>\n",
       "      <td>VTR</td>\n",
       "      <td>2020-11-06</td>\n",
       "      <td>39.639999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BENTONVILLE, Ark., Oct. 11, 2018 – Today, Walm...</td>\n",
       "      <td>1</td>\n",
       "      <td>WMT</td>\n",
       "      <td>2018-10-11</td>\n",
       "      <td>93.919998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shanghai, July 2, 2015 – Mondelēz Internationa...</td>\n",
       "      <td>0</td>\n",
       "      <td>MDLZ</td>\n",
       "      <td>2015-07-02</td>\n",
       "      <td>41.549999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Nov 05, 2018 \\nSAN FRANCISCO, Nov. 5, 2018 /PR...</td>\n",
       "      <td>1</td>\n",
       "      <td>PLD</td>\n",
       "      <td>2018-11-05</td>\n",
       "      <td>65.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  sentiment ticker  \\\n",
       "0  EL DORADO, Ark.--(BUSINESS WIRE)--Oct. 10, 201...          0    MUR   \n",
       "1  ﻿Archer Daniels Midland Company (NYSE: ADM) an...          1    ADM   \n",
       "2  MOORESVILLE, N.C., Aug. 24 /PRNewswire-FirstCa...          1    LOW   \n",
       "3  ﻿LAKE OSWEGO, Ore., Oct. 26, 2018 /PRNewswire/...          0    GBX   \n",
       "4  ﻿CME Group (NASDAQ: CME), the world's leading ...          1    CME   \n",
       "5  MADISON, N.J. and CHICAGO, Feb. 15, 2017 /PRNe...          0   RLGY   \n",
       "6  ﻿CHICAGO--(BUSINESS WIRE)--Ventas, Inc. (NYSE:...          1    VTR   \n",
       "7  BENTONVILLE, Ark., Oct. 11, 2018 – Today, Walm...          1    WMT   \n",
       "8  Shanghai, July 2, 2015 – Mondelēz Internationa...          0   MDLZ   \n",
       "9  Nov 05, 2018 \\nSAN FRANCISCO, Nov. 5, 2018 /PR...          1    PLD   \n",
       "\n",
       "        date       price  \n",
       "0 2018-10-10       31.77  \n",
       "1 2018-10-29   47.009998  \n",
       "2 2009-09-24   21.389999  \n",
       "3 2018-10-26   48.810001  \n",
       "4 2021-01-12  201.490005  \n",
       "5 2017-02-15   27.290001  \n",
       "6 2020-11-06   39.639999  \n",
       "7 2018-10-11   93.919998  \n",
       "8 2015-07-02   41.549999  \n",
       "9 2018-11-05       65.07  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns={'0':'text'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finbert",
   "language": "python",
   "name": "finbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
