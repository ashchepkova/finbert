{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook contains demo for our fine-tuned Analyst Tone model. We fine-tuned FinBERT model on 10,000 manually annotated analyst statements. You can use this script and infer sentiment on your customerized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-12T03:42:27.110781Z",
     "start_time": "2020-09-12T03:42:27.106997Z"
    }
   },
   "outputs": [],
   "source": [
    "# download pre-trained and fine-tuned weights, unzip to the working directory\n",
    "# https://gohkust-my.sharepoint.com/:u:/g/personal/imyiyang_ust_hk/EQJGiEOkhIlBqlW63TbKA3gBCYgDDcHlBCB7VTXIUMmyiA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.3 (default, Mar 27 2019, 16:54:48) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:03:02.440809Z",
     "start_time": "2020-11-14T03:03:00.771508Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "#from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertConfig\n",
    "from bertModel import BertClassification, dense_opt\n",
    "from datasets import text_dataset, financialPhraseBankDataset\n",
    "import argparse\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:03:02.447859Z",
     "start_time": "2020-11-14T03:03:02.443270Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = {0:'neutral', 1:'positive',2:'negative'}\n",
    "num_labels= len(labels)\n",
    "vocab =\"finance-uncased\"\n",
    "vocab_path = '/Users/svetlana/Downloads/analyst_tone-2/vocab' \n",
    "pretrained_weights_path ='/Users/svetlana/Downloads/analyst_tone-2/pretrained_weights'\n",
    "fine_tuned_weight_path = '/Users/svetlana/Downloads/analyst_tone-2/fine_tuned.pth'   \n",
    "max_seq_length=256\n",
    "device=torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:03:04.572108Z",
     "start_time": "2020-11-14T03:03:02.450447Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/finBERT/from git/bertModel.py:31: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(self.classifier.weight)\n"
     ]
    }
   ],
   "source": [
    "model = BertClassification(weight_path = pretrained_weights_path, \n",
    "                           num_labels=num_labels, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "bert.embeddings.word_embeddings.weight \t torch.Size([30873, 768])\n",
      "bert.embeddings.position_embeddings.weight \t torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight \t torch.Size([2, 768])\n",
      "bert.embeddings.LayerNorm.weight \t torch.Size([768])\n",
      "bert.embeddings.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight \t torch.Size([768, 768])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.encoder.layer.9.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.pooler.dense.weight \t torch.Size([768, 768])\n",
      "bert.pooler.dense.bias \t torch.Size([768])\n",
      "classifier.weight \t torch.Size([3, 768])\n",
      "classifier.bias \t torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:03:10.199945Z",
     "start_time": "2020-11-14T03:03:04.574714Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30873, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(fine_tuned_weight_path, map_location=device))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-12T02:55:51.929515Z",
     "start_time": "2020-09-12T02:55:51.927166Z"
    }
   },
   "source": [
    "# 0 is neutral, 1 is positive, and 2 is negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:03:53.760997Z",
     "start_time": "2020-11-14T03:03:53.755690Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "text_path = '/Users/svetlana/Desktop/Creation/*'\n",
    "files = glob.glob(text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        sentences = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿Hitachi and SBI, the largest state-owned comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enter into Joint Venture to accelerate Digital...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Partnership to establish a state-of-the-art ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and future ready digital payments platform\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MUMBAI, October 29, 2018 --- Hitachi, Ltd. (TS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It is planned that Hitachi Payments will inves...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Financial services market in India is making r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SBI, as the largest state-owned commercial ban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hitachi Payments empowers financial institutio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Through this joint venture, Hitachi enters a n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Hitachi will provide wide range of services co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Hitachi has also been contributing to digitali...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0\n",
       "0   ﻿Hitachi and SBI, the largest state-owned comm...\n",
       "1   enter into Joint Venture to accelerate Digital...\n",
       "2   Partnership to establish a state-of-the-art ca...\n",
       "3        and future ready digital payments platform\\n\n",
       "4   MUMBAI, October 29, 2018 --- Hitachi, Ltd. (TS...\n",
       "5   It is planned that Hitachi Payments will inves...\n",
       "6   Financial services market in India is making r...\n",
       "7   SBI, as the largest state-owned commercial ban...\n",
       "8   Hitachi Payments empowers financial institutio...\n",
       "9   Through this joint venture, Hitachi enters a n...\n",
       "10  Hitachi will provide wide range of services co...\n",
       "11  Hitachi has also been contributing to digitali..."
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(sentences)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\ufeffHitachi and SBI, the largest state-owned commercial bank in India,\\n',\n",
       "       'enter into Joint Venture to accelerate Digital Payments in India\\n',\n",
       "       'Partnership to establish a state-of-the-art card acceptance\\n',\n",
       "       'and future ready digital payments platform\\n',\n",
       "       'MUMBAI, October 29, 2018 --- Hitachi, Ltd. (TSE: 6501, \"Hitachi\") today announced that Hitachi Payment Services Pvt. Ltd. (\"Hitachi Payments\"), a wholly-owned subsidiary based in India of Hitachi, and State Bank of India (\"SBI\") have signed a definitive agreement to enter into a joint venture for the establishment of a state-of-the-art card acceptance and future ready digital payments platform for India.\\n',\n",
       "       'It is planned that Hitachi Payments will invest [26%] to SBI Payment Services Pvt. Ltd. (\"SBI Payment\"), a wholly-owned subsidiary of SBI, and through this investment, SBI Payment will be a joint venture between both parties. Both parties will proceed to apply for regulatory approvals. \\n',\n",
       "       'Financial services market in India is making remarkable progress led by economic growth, Financial Inclusion policy*1 and Digital India initiatives. Bank account holders have increased substantially in the past few years and as a result, banking transactions on ATMs and digital transactions have correspondingly increased dramatically.\\n',\n",
       "       'SBI, as the largest state-owned commercial bank in India, has more than 420 million customers and more than 6,00,000 POS*2 terminals and is today the largest merchant acquirer in the market in terms of terminals through its subsidiary SBI Payment.\\n',\n",
       "       'Hitachi Payments empowers financial institutions with a comprehensive array of technology-led cash and digital payment solutions such as ATM managed services*3 and POS processing services*4. It has over 55,000 ATMs and 850,000 POS devices (including Mobile POS) under management in India. Hitachi Payments has been providing deployment, technology and management services for the card and digital acceptance payment network of SBI since 2011.\\n',\n",
       "       \"Through this joint venture, Hitachi enters a new field of business which is the merchant acquiring business*5 in India. Hitachi will contribute to the development and expansion of digital payments service business in India by creating a digital payments platform that will enable better convenience and quality through integrating our individual strengths, SBI's large customer base, branch network, brand trust and Hitachi's capability and know-how of digital payments and state-of-the-art digital technologies such as Big Data Analytics and Artificial Intelligence (AI).\\n\",\n",
       "       'Hitachi will provide wide range of services contributing to \"Digital India\" by creating innovative solutions with \"Lumada\". With this JV, Hitachi will accelerate digitalization of financial services in India by linking up digital payments platform to state-of-the-art digital technologies of \"Lumada\", and also will provide solutions for mass transit sector and e-commerce businesses. \\n',\n",
       "       'Hitachi has also been contributing to digitalize governmental and educational services such as \"e-Governance\" and \"e-Education\" mainly through Hitachi MGRM Net*6 which was established in April 2018 and provides IT services in India. and leverage considerable knowledge for our global business.\\n'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages (4.1.1)\n",
      "Requirement already satisfied: requests in /Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages (from transformers) (2.25.0)\n",
      "Requirement already satisfied: filelock in /Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: numpy in /Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in /Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages (from transformers) (0.9.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages (from transformers) (4.54.1)\n",
      "Requirement already satisfied: sacremoses in /Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: packaging in /Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages (from transformers) (20.7)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages (from requests->transformers) (1.26.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: click in /Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages (from transformers) (4.54.1)\n",
      "Requirement already satisfied: six in /Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages (from transformers) (2020.11.13)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:03:54.277505Z",
     "start_time": "2020-11-14T03:03:54.217928Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer(vocab_file = vocab_path, do_lower_case = True, \n",
    "                          do_basic_tokenize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2179: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "for sent in sentences:\n",
    "    encoded_data = tokenizer.batch_encode_plus(\n",
    "    sentences, add_special_tokens = True,\n",
    "    return_attention_mask = True,\n",
    "    max_length = max_seq_length,\n",
    "    pad_to_max_length = True,\n",
    "    return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.empty(len(sentences), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlana/opt/miniconda3/envs/finbert/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "input_ids = encoded_data['input_ids']\n",
    "attention_masks = encoded_data['attention_mask']\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5764607523034234880, 1152930294905439388,                  11,\n",
       "                          0,                   0,                   0,\n",
       "                          0,                   0,                   0,\n",
       "                          0,                   0,                   0])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(input_ids, attention_masks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    sampler = SequentialSampler(dataset),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataloader_):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for batch in dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels': batch[2],\n",
    "                 }\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        outputs = F.softmax(outputs,dim=1)\n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        \n",
    "        predictions.append(outputs)\n",
    "        \n",
    "    predictions = np.concatenate(predictions, axis = 0)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99121726e-01, 6.42632367e-04, 2.35585467e-04],\n",
       "       [9.99090672e-01, 7.70008308e-04, 1.39302560e-04],\n",
       "       [9.93162036e-01, 6.80811796e-03, 2.98820760e-05],\n",
       "       [1.04347067e-02, 9.89528000e-01, 3.73049334e-05],\n",
       "       [9.98422384e-01, 1.56004529e-03, 1.75645400e-05],\n",
       "       [9.99921560e-01, 5.48552198e-05, 2.35760435e-05],\n",
       "       [1.70250829e-07, 9.99999762e-01, 1.29963453e-07],\n",
       "       [8.82389426e-01, 1.17605723e-01, 4.78871607e-06],\n",
       "       [5.92404723e-01, 4.07585800e-01, 9.46625187e-06],\n",
       "       [2.07257335e-06, 9.99997973e-01, 2.34200428e-08],\n",
       "       [2.07182378e-01, 7.92782605e-01, 3.50150985e-05],\n",
       "       [1.18270125e-02, 9.88171220e-01, 1.77317634e-06]], dtype=float32)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = predict(dataloader_ = dataloader)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0.4166666666666667\n"
     ]
    }
   ],
   "source": [
    "positives = 0\n",
    "for i in range(len(pred)):\n",
    "    pred_labels_i = np.argmax(pred[i]).flatten()\n",
    "    if pred_labels_i[0] == 1:\n",
    "        positives+=1\n",
    "print(positives) \n",
    "print(positives / len(pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finbert",
   "language": "python",
   "name": "finbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
